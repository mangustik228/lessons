{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Å—Å—ã–ª–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é : https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –õ–æ–∫–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–µ–º –≤ Jupiter\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Python Spark SQL basic example\")\\\n",
    "    .getOrCreate()\n",
    "    # —Å–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç Spark-—Å–µ—Å—Å–∏–∏, –æ–±—Ä–∞—â–∞—è—Å—å –∫ –æ–±—ä–µ–∫—Ç—É builder, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç —Å–µ—Å—Å–∏—é, —É—á–∏—Ç—ã–≤–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "# —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —Ö–æ—Ç–∏–º –∑–∞–ø—É—Å—Ç–∏—Ç—å Spark –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ\n",
    "# –∑–∞–¥–∞—ë–º –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞—à–µ–≥–æ Spark-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è\n",
    "# —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞ —Å–µ—Å—Å–∏–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7])\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° RDD(–∞–Ω–≥–ª. resilient distributed dataset) ‚Äî –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∏–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è. –¢–∏–ø —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø–æ —É–∑–ª–∞–º –∫–ª–∞—Å—Ç–µ—Ä–∞, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º–æ–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –í–∞—Ä–∏–∞–Ω—Ç 1\n",
    "rdd2 = spark.sparkContext.emptyRDD()\n",
    " # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –í–∞—Ä–∏–∞–Ω—Ç 2\n",
    "rdd2 = spark.sparkContext.parallelize([])\n",
    " # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–∑ csv\n",
    "rdd3 = spark.sparkContext.textFile('C:/tmp/files/text01.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition(val:int) –ò–∑–º–µ–Ω–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
    "rdd.repartition(5)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoalescedRDD[6] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce() –¢–æ–ª—å–∫–æ —É–º–µ–Ω—å—à–∞–µ—Ç\n",
    "rdd.coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–∏–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "# map –∏ –≤ –∞—Ñ—Ä–∏–∫–µ map\n",
    "rdd2 = rdd.map(lambda x: x + 2)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h | e | l | l | o |   | w | o | r | l | d | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize('hello world')\n",
    "# –ú–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –Ω–µ—Å–∫–æ–ª—å –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "rdd3 = rdd.flatMap(lambda x: x.split(','))\n",
    "for i in rdd3.collect():\n",
    "    print(i, end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 34, 6, 6, 8, 10]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "rdd = spark.sparkContext.parallelize([1,2,34,5,6,6,7,8,9,10])\n",
    "rdd4 = rdd.filter(lambda x: x%2 == 0)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['world', 'hello']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–º\n",
    "rdd = spark.sparkContext.parallelize(['hello','vs','vasya','world'])\n",
    "rdd2 = spark.sparkContext.parallelize(['jopa','spark', 'hello','world'])\n",
    "intersection = rdd.intersection(rdd2)\n",
    "intersection.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_reduce.collect() = [('beer', 5), ('vodka', 16), ('milk', 0)]\n",
      "rdd_group.map(lambda x: (x[0], list(x[1]))).collect() = [('beer', [1, 5]), ('vodka', [2, 8]), ('milk', [0])]\n"
     ]
    }
   ],
   "source": [
    "# distinct() - —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "# union() - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π rdd –∏–∑ –¥–≤—É—Ö –¥—Ä—É–≥–∏—Ö\n",
    "# sortByKey() - —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É\n",
    "# sortBy(function) - —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ñ-—Ü–∏–∏\n",
    "# join()\n",
    "# reduceByKey(func) - –∞–Ω–∞–ª–æ–≥ groupby\n",
    "# groupByKey() - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –æ–±—ä–µ–∫—Ç(–ø–æ—Ö–æ–∂ –Ω–∞ reduceByKey)\n",
    "rdd = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        ('beer',1),\n",
    "        ('vodka',2),\n",
    "        ('beer',5),\n",
    "        ('milk',0),\n",
    "        ('vodka',8)\n",
    "    ]\n",
    ")\n",
    "rdd_reduce = rdd.reduceByKey(lambda x, y: x * y)\n",
    "rdd_group = rdd.groupByKey()\n",
    "print(f'{rdd_reduce.collect() = }')\n",
    "print(f'{rdd_group.map(lambda x: (x[0], list(x[1]))).collect() = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–µ–π—Å—Ç–≤–∏—è (actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|–º–µ—Ç–æ–¥|—á—Ç–æ –¥–µ–ª–∞–µ—Ç|–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è|\n",
    "|--|--|--|\n",
    "|`collect()`|–Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ–±—Ä–∞—Ç—å –≤—Å–µ —á–∞—Å—Ç–∏ RDD|—Å–º.–≤—ã—à–µ|\n",
    "|`count()`|–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥–æ—á–µ—Ä–Ω–µ–π RDD.|`count(rdd2)` -> 5|\n",
    "|`take(n)`|–ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π `head()` –≤ –ø–∞–Ω–¥–∞—Å|`rdd.take(2)`|\n",
    "|`countByValue()`|–ú–µ—Ç–æ–¥ –ø–æ—Ö–æ–∂ –Ω–∞ reduceByKey(), –Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è, –∫–æ–≥–¥–∞ RDD —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å. –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω value_counts()–≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ pandas.|`rdd.countByValue()`->`[(1,8),(2,3),(3,15)]`|\n",
    "|`countByKey()`|–≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ç–æ–¥—É reduceByKey(), —Ç–∞–∫ –∫–∞–∫ —Å—É–º–º–∏—Ä—É–µ—Ç –ø–æ –∫–ª—é—á—É. –û—Ç–ª–∏—á–∏–µ –≤ —Ç–æ–º, —á—Ç–æ reduceByKey() ‚Äî —ç—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—É—é RDD, –∞ countByKey() ‚Äî —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∞.||\n",
    "|`reduce(function)`|—è–≤–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏. –û–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –µ—ë –∫–æ –≤—Å–µ–π RDD —Ü–µ–ª–∏–∫–æ–º.|`sc.parallelize([1, 2, 3, 4, 5]).reduce(add)` -> 15|\n",
    "|``|||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∏–Ω—É—Å RDD API: \n",
    "- –Ω–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∞ –Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç `MapReduce` –∏ –Ω–µ –æ—Å–æ–±–æ –ø–æ–Ω—è—Ç–Ω–∞ –Ω–∞—á–∏–Ω–∞—é—â–∏–º\n",
    "- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ —Å–ª–∞–±–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –Ω–æ —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏: –ø–ª–æ—Ö–æ.\n",
    "–û—Ç RDD DataFrame —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞–ª –º–Ω–æ–≥–æ –ø–æ–ª–µ–∑–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
    "- –ü—Ä–æ—Ü–µ—Å—Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–Ω–µ—á–Ω–æ–≥–æ DataFrame –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä—è–º–æ –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏.\n",
    "- –ö–∞–∫ –∏ RDD, DataFrame –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –º–∞–Ω–µ—Ä–µ –∑–∞ —Å—á—ë—Ç –º–µ—Ö–∞–Ω–∏–∑–º–∞ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "- –í—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ DataFrame API —Ç–æ–∂–µ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –ª–µ–Ω–∏–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –∞ –º–µ—Ç–æ–¥—ã —Ç–∞–∫–∂–µ –¥–µ–ª—è—Ç—Å—è –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è.\n",
    "- DataFrame –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ RDD, –Ω–∞—Å–ª–µ–¥—É—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å RDD API, –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ DataFrame API –¥–µ–ª—è—Ç—Å—è –Ω–∞ –¥–≤–∞ —Ç–∏–ø–∞:\n",
    "- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–∞–Ω–≥–ª. transformations) —Å–æ–∑–¥–∞—é—Ç –æ–±—ä–µ–∫—Ç—ã DataFrame –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –Ω–æ –Ω–µ –∑–∞–ø—É—Å–∫–∞—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.\n",
    "- –î–µ–π—Å—Ç–≤–∏—è (–∞–Ω–≥–ª. actions) ‚Äî —è–≤–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–∑–¥–∞–Ω–∏–µ df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–Ω–µ –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç \n",
    "- –ü—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ `spark.read.csv('datasets/cal_housing_data.csv', schema=schema)`\n",
    "- —Å–æ–∑–¥–∞—Ç—å —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `createDataFrame()`\n",
    "- –∏–∑ pd.DataFrame: `spark.createDataFrame(pandas_df) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- column_1: string (nullable = true)\n",
      " |-- column_2: long (nullable = true)\n",
      " |-- column_3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('val_1_1', 1, 123.22),\n",
    "        ('val_2_1', 2, 124.22),\n",
    "        ('val_3_1', 3, 125.22),]\n",
    "columns = ['column_1', 'column_2', 'column_3']\n",
    "# schema - –ø–µ—Ä–µ–¥–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫\n",
    "df = spark.createDataFrame(data=data, schema=columns) \n",
    "df.printSchema() # –í—ã–≤–µ—Å—Ç–∏ —Å—Ö–µ–º—É"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞—Ç—å schema —è–≤–Ω–æ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"longitude\", FloatType(), nullable=True),\n",
    "    StructField(\"latitude\", FloatType(), nullable=True),\n",
    "    StructField(\"median_age\", FloatType(), nullable=True),\n",
    "    StructField(\"total_rooms\", FloatType(), nullable=True),\n",
    "    StructField(\"total_bdrms\", FloatType(), nullable=True),\n",
    "    StructField(\"population\", FloatType(), nullable=True),\n",
    "    StructField(\"households\", FloatType(), nullable=True),\n",
    "    StructField(\"median_income\", FloatType(), nullable=True),\n",
    "    StructField(\"median_house_value\", FloatType(), nullable=True)]\n",
    ")\n",
    "data = spark.read.csv('datasets/cal_housing_data.csv', schema=schema)\n",
    "data.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–µ–π—Å—Ç–≤–∏—è Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|–ú–µ—Ç–æ–¥|–ß—Ç–æ –¥–µ–ª–∞–µ—Ç|–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ|\n",
    "|--|--|--|\n",
    "|`.show(5)`|–∞–Ω–∞–ª–æ–≥ .head(5)||\n",
    "|`take(n)`|–∑–∞–ø—É—Å–∫–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π|–∑–∞–ø—É—Å–∫ —Å DataFrame –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç n-–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫.|\n",
    "|`toPandas()`|–ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –≤ pd.DataFrame|–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–µ—Ç–æ–¥–æ–º toPandas() –Ω—É–∂–Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ. –û–Ω –∑–∞–ø—É—Å–∫–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ –ø–ª–∞–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π, –∏ –µ—Å–ª–∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤–µ—Å–∏—Ç –±–æ–ª—å—à–µ, —á–µ–º –ø–æ–∑–≤–æ–ª—è—é—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Spark-—Å–µ—Å—Å–∏–∏ spark.driver.maxResultSize –∏ spark.driver.memory, —Ç–æ Spark-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è –ø–æ –ø—Ä–∏—á–∏–Ω–µ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏. –¢–∞–∫ –º–æ–∂–Ω–æ –ø–æ—Ç–µ—Ä—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞—Å—á—ë—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏.|\n",
    "|`collect()`|–∑–∞–ø—É—Å–∫–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ –ø–ª–∞–Ω–∞ |—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É –Ω–∞ –¥—Ä–∞–π–≤–µ—Ä–µ. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Å–æ–∑–¥–∞—ë—Ç—Å—è Python-—Å–ø–∏—Å–æ–∫ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ —Ç–∏–ø–∞ Row ‚Äî –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Ç–∏–ø–æ–º –¥–∞–Ω–Ω—ã—Ö Spark, –∫–æ—Ç–æ—Ä—ã–π —Ö—Ä–∞–Ω–∏—Ç –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫.|\n",
    "|`count()`|–∑–∞–ø—É—Å–∫–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ –ø–ª–∞–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π |–∏ —Å—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–∞ –¥—Ä–∞–π–≤–µ—Ä–µ.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏** ‚Äî —ç—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–º–µ–Ω—è—é—Ç —Ç–∞–±–ª–∏—Ü—É-–∏—Å—Ç–æ—á–Ω–∏–∫, –Ω–æ –Ω–µ –∑–∞–ø—É—Å–∫–∞—é—Ç –ø—Ä–æ—Ü–µ—Å—Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–ø–µ—Ä–∞—Ü–∏—è –ø–æ—è–≤–∏—Ç—Å—è –≤ –ø–ª–∞–Ω–∞—Ö –∑–∞–ø—Ä–æ—Å–∞.  \n",
    "\n",
    "**–î–ª—è –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π** —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ –≤ Spark SQL –µ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å ‚Äî `Spark Functions`. –í –Ω—ë–º —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –º–Ω–æ–≥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö, —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö, –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. \n",
    "\n",
    "|–ú–µ—Ç–æ–¥|–ß—Ç–æ –¥–µ–ª–∞–µ—Ç|–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ|\n",
    "|--|--|--|\n",
    "|`select()`|–ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä—É SELECT –≤ SQL||\n",
    "|`distinct()`|–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä—É DISTINCT –≤ SQL|1*–ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–ø–µ—Ä–∞—Ü–∏—é –¥–µ–π—Å—Ç–≤–∏—è collect() –∏–ª–∏ toPandas()     |\n",
    "|`withColumn()`|–ú–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É. |2*–í —Ç–∞–±–ª–∏—Ü–µ-–ø—Ä–∏–º–µ—Ä–µ –ø–æ—è–≤–∏—Ç—Å—è –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü|\n",
    "|`lit()`|—Å–æ–∑–¥–∞—ë—Ç —Å—Ç–æ–ª–±–µ—Ü —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º|3*–∏–∑ –º–æ–¥—É–ª—è Spark Functions|\n",
    "|`withColumnRenamed()`|–ú–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏. |4*–ù–æ–≤–æ–µ –∏–º—è —É–∫–∞–∑—ã–≤–∞—é—Ç –≤—Ç–æ—Ä—ã–º –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–µ.|\n",
    "|`toDF()`|–ú–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç —Ç–∏–ø–∞ DataFrame.|5*–ï–≥–æ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤ —Å–ª—É—á–∞—è—Ö, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫ —Å—Ä–∞–∑—É.|\n",
    "|`cast()`|–∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä—É CAST –≤ SQL|6*–º–µ–Ω—è–µ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π.|\n",
    "|`filter()`|–æ—Ç–±–æ—Ä –ø–æ —É—Å–ª–æ–≤–∏—é|7*–ï–≥–æ —Å–æ—á–µ—Ç–∞—é—Ç –≤–º–µ—Å—Ç–µ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π col() –∏–∑ –º–æ–¥—É–ª—è Spark Functions:|\n",
    "|||8**–ß—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —É—Å–ª–æ–≤–∏—é, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏–≤—ã—á–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å pandas|\n",
    "|`like()`|–ê–Ω–∞–ª–æ–≥ `LIKE` in SQL||\n",
    "|`rlike()`|||\n",
    "|`startswith()`|||\n",
    "|`endswith()`|||\n",
    "|`contains()`|||\n",
    "|` orderBy()`|||\n",
    "|`join()`|–ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω join() –≤ pandas||\n",
    "|`broadcast()`||–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ —Å–æ–µ–¥–∏–Ω–∏—Ç—å –¥–≤–µ —Ç–∞–±–ª–∏—Ü—ã. –†–∞–∑–º–µ—Ä –ø–µ—Ä–≤–æ–π ‚Äî –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å—Ç—Ä–æ–∫, –∞ –≤—Ç–æ—Ä–æ–π ‚Äî –≤—Å–µ–≥–æ 100‚Äì200 —Å—Ç—Ä–æ–∫.|\n",
    "\n",
    "```python\n",
    "# 1* \n",
    "casesDist = cases.select('province').distinct()\n",
    "# 2* \n",
    "casesNew = cases.withColumn('confirmedNew', F.col('confirmed') + 99)\n",
    "# 3* \n",
    "casesPred = cases.withColumn('prediction', F.lit(1))\n",
    "# 4*\n",
    "cases = cases.withColumnRenamed('infection_case', 'infection_source')\n",
    "# 5*\n",
    "cases = cases.toDF(*['case_id', 'province', 'city', 'group', 'infection_case', 'latitude', 'longitude'])\n",
    "# 6*\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType \n",
    "cases = cases.withColumn('group', F.col('group').cast(IntegerType()))\n",
    "# 7*\n",
    "cases.filter(F.col('province') == 'Seoul').count() \n",
    "# 8*\n",
    "cases.filter(F.col('province') != 'Seoul').count() \n",
    "cases.filter(~(F.col('province') == 'Seoul')).count()\n",
    "cases.filter((cases.province  == 'Seoul') & (cases.city  == 'Guro-gu')).toPandas() \n",
    "```\n",
    "–ë–æ–ª—å—à–µ –º–µ—Ç–æ–¥–æ–≤ —Å–º. –≤ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#dataframe-apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca06022eeaaba41f8106b5e7859db2308ae98973a4e7711a4dd80a8bd4150e2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
