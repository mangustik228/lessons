{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÑÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑŽ : https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ð² Jupiter\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Python Spark SQL basic example\")\\\n",
    "    .getOrCreate()\n",
    "    # ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ð±ÑŠÐµÐºÑ‚ Spark-ÑÐµÑÑÐ¸Ð¸, Ð¾Ð±Ñ€Ð°Ñ‰Ð°ÑÑÑŒ Ðº Ð¾Ð±ÑŠÐµÐºÑ‚Ñƒ builder, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ ÑÐµÑÑÐ¸ÑŽ, ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸\n",
    "# ÑÐ²Ð½Ð¾ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ñ…Ð¾Ñ‚Ð¸Ð¼ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Spark Ð² Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ\n",
    "# Ð·Ð°Ð´Ð°Ñ‘Ð¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð½Ð°ÑˆÐµÐ³Ð¾ Spark-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ\n",
    "# Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° ÑÐµÑÑÐ¸Ð¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7])\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ ÑÐµÑÑÐ¸Ð¸\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ RDD(Ð°Ð½Ð³Ð». resilient distributed dataset) â€” Ð¾Ñ‚ÐºÐ°Ð·Ð¾ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ñ‹Ð¹ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½Ð½Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¸Ð»Ð¸ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ. Ð¢Ð¸Ð¿ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÑŽÑ‰Ð¸Ð¹ ÑÐ¾Ð±Ð¾Ð¹ Ð½Ð°Ð±Ð¾Ñ€ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð², Ñ€Ð°Ð·Ð´ÐµÐ»Ñ‘Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ ÑƒÐ·Ð»Ð°Ð¼ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°, Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð¶Ð½Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1\n",
    "rdd2 = spark.sparkContext.emptyRDD()\n",
    " # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2\n",
    "rdd2 = spark.sparkContext.parallelize([])\n",
    " # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð¸Ð· csv\n",
    "rdd3 = spark.sparkContext.textFile('C:/tmp/files/text01.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÐŸÐ°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¹\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition(val:int) Ð˜Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¹\n",
    "rdd.repartition(5)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoalescedRDD[6] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce() Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ°ÐµÑ‚\n",
    "rdd.coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð¢Ð¸Ð¿Ñ‹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "# map Ð¸ Ð² Ð°Ñ„Ñ€Ð¸ÐºÐµ map\n",
    "rdd2 = rdd.map(lambda x: x + 2)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h | e | l | l | o |   | w | o | r | l | d | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize('hello world')\n",
    "# ÐœÐ¾Ð¶ÐµÑ‚ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²\n",
    "rdd3 = rdd.flatMap(lambda x: x.split(','))\n",
    "for i in rdd3.collect():\n",
    "    print(i, end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 34, 6, 6, 8, 10]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ\n",
    "rdd = spark.sparkContext.parallelize([1,2,34,5,6,6,7,8,9,10])\n",
    "rdd4 = rdd.filter(lambda x: x%2 == 0)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['world', 'hello']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÐŸÐµÑ€ÐµÑÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð¼\n",
    "rdd = spark.sparkContext.parallelize(['hello','vs','vasya','world'])\n",
    "rdd2 = spark.sparkContext.parallelize(['jopa','spark', 'hello','world'])\n",
    "intersection = rdd.intersection(rdd2)\n",
    "intersection.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_reduce.collect() = [('beer', 5), ('vodka', 16), ('milk', 0)]\n",
      "rdd_group.map(lambda x: (x[0], list(x[1]))).collect() = [('beer', [1, 5]), ('vodka', [2, 8]), ('milk', [0])]\n"
     ]
    }
   ],
   "source": [
    "# distinct() - ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ\n",
    "# union() - ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð½Ð¾Ð²Ð¾Ð¹ rdd Ð¸Ð· Ð´Ð²ÑƒÑ… Ð´Ñ€ÑƒÐ³Ð¸Ñ…\n",
    "# sortByKey() - ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ ÐºÐ»ÑŽÑ‡Ñƒ\n",
    "# sortBy(function) - ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ Ñ„-Ñ†Ð¸Ð¸\n",
    "# join()\n",
    "# reduceByKey(func) - Ð°Ð½Ð°Ð»Ð¾Ð³ groupby\n",
    "# groupByKey() - Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸Ð½Ñ‚ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ð¹ Ð¾Ð±ÑŠÐµÐºÑ‚(Ð¿Ð¾Ñ…Ð¾Ð¶ Ð½Ð° reduceByKey)\n",
    "rdd = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        ('beer',1),\n",
    "        ('vodka',2),\n",
    "        ('beer',5),\n",
    "        ('milk',0),\n",
    "        ('vodka',8)\n",
    "    ]\n",
    ")\n",
    "rdd_reduce = rdd.reduceByKey(lambda x, y: x * y)\n",
    "rdd_group = rdd.groupByKey()\n",
    "print(f'{rdd_reduce.collect() = }')\n",
    "print(f'{rdd_group.map(lambda x: (x[0], list(x[1]))).collect() = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð”ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ (actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Ð¼ÐµÑ‚Ð¾Ð´|Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚|Ð¿Ð¾ÑÐ»ÐµÐ´ÑÑ‚Ð²Ð¸Ñ|\n",
    "|--|--|--|\n",
    "|`collect()`|Ð¯Ð²Ð½Ð¾ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÑÐ¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð²ÑÐµ Ñ‡Ð°ÑÑ‚Ð¸ RDD|ÑÐ¼.Ð²Ñ‹ÑˆÐµ|\n",
    "|`count()`|Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÑ‚ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð² Ð´Ð¾Ñ‡ÐµÑ€Ð½ÐµÐ¹ RDD.|`count(rdd2)` -> 5|\n",
    "|`take(n)`|ÐœÐµÑ‚Ð¾Ð´ Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ñ‹Ð¹ `head()` Ð² Ð¿Ð°Ð½Ð´Ð°Ñ|`rdd.take(2)`|\n",
    "|`countByValue()`|ÐœÐµÑ‚Ð¾Ð´ Ð¿Ð¾Ñ…Ð¾Ð¶ Ð½Ð° reduceByKey(), Ð½Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ÑÑ, ÐºÐ¾Ð³Ð´Ð° RDD ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð· Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð¾ÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ. ÐœÐµÑ‚Ð¾Ð´ Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡ÐµÐ½ value_counts()Ð² Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐµ pandas.|`rdd.countByValue()`->`[(1,8),(2,3),(3,15)]`|\n",
    "|`countByKey()`|Ð­Ñ‚Ð¾ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ñƒ reduceByKey(), Ñ‚Ð°Ðº ÐºÐ°Ðº ÑÑƒÐ¼Ð¼Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾ ÐºÐ»ÑŽÑ‡Ñƒ. ÐžÑ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð² Ñ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾ reduceByKey() â€” ÑÑ‚Ð¾ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð½Ð¾Ð²ÑƒÑŽ RDD, Ð° countByKey() â€” ÑÑ‚Ð¾ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð°.||\n",
    "|`reduce(function)`|ÑÐ²Ð½Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð½Ð° ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ðµ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸. ÐžÐ½ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÑŽÑ‰ÑƒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ ÐµÑ‘ ÐºÐ¾ Ð²ÑÐµÐ¹ RDD Ñ†ÐµÐ»Ð¸ÐºÐ¾Ð¼.|`sc.parallelize([1, 2, 3, 4, 5]).reduce(add)` -> 15|\n",
    "|``|||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca06022eeaaba41f8106b5e7859db2308ae98973a4e7711a4dd80a8bd4150e2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
